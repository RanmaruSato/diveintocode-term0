{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰とは \n",
    "ロジスティック回帰 は線形分離可能なデータの境界線を学習によって見つけてデータの分類を行なう手法。\n",
    "特徴としては境界線が直線になること。そのため、二項分類などクラスの少ないデータに用いられる。 また、データがクラスに分類される確率も計算することが可す。これらの特徴から主に「天気予報の降水確率」など、分類される確率を知りたい時に用いられる。\n",
    "\n",
    "\n",
    "# 交差エントロピー関数とは  \n",
    "情報の最小単位を1bitとして、情報量は確率の関数として,f(p)とあらわせるとする。\n",
    "独立な事象１と２が起きた時の確率Pは、それぞれ\n",
    "P = P1* P2であらわせる。\n",
    "この時の情報量は\n",
    "f(P)=f(P1)*f(P2)とすると、左辺の単位はビット、右辺の単位は（ビット）^2となって一致しなくなる。\n",
    "そこで\n",
    "f(P) = f(P1)+f(P2)となると単位が一致する。\n",
    "つまり、事象１と事象２が起きた時の情報量は事象１が起きたという情報と、事象２が起きたという情報の和であらわせるべきということ。\n",
    "つまり、f(P₁* P₂)＝f(P₁)＋f(P₂）という性質を持つことになる。\n",
    "これは、log(P₁×P₂)=log(P₁)＋log(P₂）という対数関数の性質である。\n",
    "f (p) = log(p)と表したいが、これだとpが小さくなるとf(p)が小さくねったしまう。\n",
    "確率が小さい事象が起きた時の情報量が大きいと考える方が自然なので\n",
    "f(p)＝-log(p)とマイナスをつける。情報エントロピーは何かデータを得た時の驚き具合と表現される。\n",
    "これが自己情報量とあらわせる。\n",
    "通常のエントロピーとはp×log(p)のようにlogの中身と外側に同じ変数が使われているのが普通のエントロピーである。\n",
    "それに対して、交差エントロピーとは、t×log(y)のようにlogの中身と外側に異なる変数が使われているものである。\n",
    "交差エントロピーとは、情報量の期待値であり、確率*実現値で定義できる。\n",
    "\n",
    "つまり、\n",
    "$ { \\displaystyle\n",
    "\\begin{eqnarray*}\n",
    "交差エントロピー&=&期待値 \\\\\n",
    " &=&\\sum_{x} 確率 * 実現値 \\\\\n",
    " &=&\\sum_{x} 確率 * 情報量 \\\\\n",
    " &=&\\sum_{x} p(x)*(-\\log q(x)) \\\\\n",
    " &=&-\\sum_{x} p(x)\\log q(x)\n",
    "\\end{eqnarray*}\n",
    "}\n",
    "$\n",
    "２値分類の誤差関数は、$ {\\log L = -\\sum_{i=1}^N \\left(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log(1-f_\\theta(x^{(i)}))\\right) $\n",
    "\n",
    "# シグモイド関数とは\n",
    "$ \\displaystyle f(x) = \\frac{1}{1+\\exp (-x)} $\n",
    "以上の関数は 0 < f(x) < 1の範囲となる。\n",
    "この関数に入力させることで、確率と同じような性質を与えることができる。\n",
    "# 正則化とは  \n",
    "学習をする際に、偏りすぎたデータにまで必要以上に対応してしまうと、過学習の状態に陥ることがある。\n",
    "過学習の状態は、与えた学習データに対して、小さな誤差となるモデルが構築できている。ただし、ごく一部の例外的なデータに過度に対応したモデルとなっているために、構築した学習モデルを未知データに適用すると必ずしも適切な予測値を返さない状態となる。\n",
    "学習データの中のごく一部の例外的なデータに過度に適用したモデルが構築されている状態ということで過学習という。\n",
    "そのため、極端な重みのデータに対してペナルティを与える正則化が用いられる。正則化とは、モデルを学習する際に、複雑さが増すことに対するペナルティを設け、このペナルティを訓練誤差に加えた量がもっとも小さくなる学習モデルを求めるように汎化性能を高めようとするもの。\n",
    "機械学習では、L1正則化とL2正則化が一般的に使用される。\n",
    "L1正則化は、ペナルティとして、学習モデルのパラメーターの絶対値の総和に用いるもので、L2正則化は、ペナルティとして学習モデルのパラメータの２乗の総和を用いるもので、以下のような特徴を持つ。\n",
    "- L1正則化　特定のデータの重みを0にする事で、不要なデータを削除する\n",
    "- L2正則化　データの大きさに応じて0に近づけて、滑らかなモデルとする\n",
    "\n",
    "# ロジスティック回帰では平均二乗誤差ではなく交差エントロピー誤差を使う理由  \n",
    "凸最適化をするため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://aidiary.hatenablog.com/entry/20140415/1397570262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    y = 1.0 / (1.0 + np.exp(-X))\n",
    "    return y      \n",
    "\n",
    "def y_hat(X,theta):\n",
    "    #y =  sigmoid(np.dot(theta,　X.T))\n",
    "    #仮定関数は、シグモイド関数の引数にθxを入れたもの\n",
    "    #theta= 4列\n",
    "    #X = 100*4\n",
    "    #(1*4 ) *(4*100)\n",
    "    # 100\n",
    "    y = sigmoid(np.dot(theta,X.T))\n",
    "    return y\n",
    "\n",
    "def compute_cost(X,y,theta,λ= 0.01):\n",
    "    #numpy配列は計算が早いのでここで変換\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    #第１項\n",
    "    one_one = np.dot((-1 * y), np.log(y_hat(X,theta)))\n",
    "    one_two = np.dot((1-y),np.log(1-  y_hat(X,theta)))\n",
    "    one = 1/len(X)*(one_one- one_two)\n",
    "    #第２項\n",
    "    two = (λ/2*len(X))*sum(theta**2)\n",
    "    return one+two\n",
    "\n",
    "\n",
    "def  gradient_descent(X,y,theta):\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        lr = 0.0001\n",
    "       #訓練データの誤差\n",
    "        past_costs = []\n",
    "        #重み\n",
    "        past_thetas = []\n",
    "        #平均二乗和誤差を計算する\n",
    "        loss = compute_cost(X,y,theta)\n",
    "        #追加\n",
    "        past_costs.append(loss)\n",
    "        #追加\n",
    "        past_thetas.append(theta)\n",
    "\n",
    "        for i in range(5000):\n",
    "            #仮定関数\n",
    "            y_h=y_hat(X,theta)\n",
    "\n",
    "            #パラメータの更新式\n",
    "            theta = theta - lr *(1/len(y))*(X.T.dot(y_h-y))\n",
    "            # 訓練データの誤差を計算する\n",
    "            loss = compute_cost(X,y,theta)\n",
    "            #訓練データの誤差を格納する\n",
    "            past_costs.append(loss)\n",
    "            past_thetas.append(theta)\n",
    "\n",
    "        loss = past_costs\n",
    "        theta = past_thetas\n",
    "\n",
    "        return  past_thetas,past_costs\n",
    "    \n",
    "def predict_probs(X,theta):\n",
    "    return  y_hat(X,past_thetas[-1])\n",
    "\n",
    "\n",
    "def predict(X,theta,threshold=0.5):\n",
    "    pred = y_hat(X,theta)\n",
    "    for i in range(pred.size):\n",
    "        if pred[i]>= 0.5:\n",
    "            pred[i] = 1\n",
    "        elif pred[i] < 0.5:\n",
    "            pred[i] = 0\n",
    "    return pred.astype('int')\n",
    "\n",
    "def plot_loss_curve(past_costs):\n",
    "    pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "    plt.title('LossCurve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('../iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = (iris['Species'][iris['Species']=='Iris-versicolor'].index)\n",
    "index = index.append( iris['Species'][iris['Species']=='Iris-virginica'].index)\n",
    "iris = iris.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'Iris-versicolor':0,'Iris-virginica':1}\n",
    "iris.Species = iris.Species.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.drop(['Id','Species'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = iris.Species\n",
    "theta = np.random.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " past_thetas,past_costs = gradient_descent(X,y,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(past_costs):\n",
    "    pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "    plt.title('LossCurve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranmarusato/anaconda/lib/python3.6/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYAXyxyThAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#正解率\n",
    "def accuracy_rate(y, t):\n",
    "    #データ型の確認の条件式\n",
    "    return np.sum(y == t)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48262847, 0.53073288, 0.4961828 , 0.5030165 , 0.50339156,\n",
       "       0.51532509, 0.55479786, 0.49936049, 0.47298556, 0.55586976,\n",
       "       0.47121726, 0.54838482, 0.42630514, 0.51350789, 0.53299324,\n",
       "       0.49530326, 0.5614785 , 0.46218295, 0.48777081, 0.47873499,\n",
       "       0.59897184, 0.49845231, 0.49540412, 0.47762101, 0.4857745 ,\n",
       "       0.49517348, 0.47091382, 0.53099346, 0.53553547, 0.46679988,\n",
       "       0.47939906, 0.46495887, 0.49426306, 0.53536135, 0.57171949,\n",
       "       0.5769745 , 0.50819839, 0.45822861, 0.53435488, 0.51369904,\n",
       "       0.50058425, 0.51963906, 0.48812705, 0.48880622, 0.51757793,\n",
       "       0.5131168 , 0.5230416 , 0.49619709, 0.51194099, 0.51850154,\n",
       "       0.67455354, 0.59067594, 0.56379997, 0.55680759, 0.60969237,\n",
       "       0.53247281, 0.60073366, 0.49935698, 0.51323621, 0.64613985,\n",
       "       0.59598778, 0.55855112, 0.58221604, 0.60088975, 0.66680148,\n",
       "       0.64259396, 0.55257055, 0.58373883, 0.53405782, 0.49421986,\n",
       "       0.61530248, 0.62182983, 0.50054337, 0.55174722, 0.60120422,\n",
       "       0.52296795, 0.56294856, 0.57775244, 0.59130532, 0.48333226,\n",
       "       0.50567172, 0.54580298, 0.6059903 , 0.50984157, 0.49032826,\n",
       "       0.56152926, 0.66859139, 0.56298534, 0.58360414, 0.58311558,\n",
       "       0.6351314 , 0.61475998, 0.59067594, 0.61872715, 0.65813239,\n",
       "       0.61881436, 0.55550361, 0.58488295, 0.66103493, 0.58635054])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probs(X,past_thetas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(X,past_thetas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各評価指標の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([50, 52, 57, 58, 60, 62, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 79,\n",
       "            80, 81, 82, 87, 92, 93, 97],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred==y].index &y[pred==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TruePositive\n",
    "TP = len(y[pred==y].index &y[pred==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113,\n",
       "            114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127,\n",
       "            128, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "            143, 144, 145, 146, 147, 148, 149],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred==y].index &y[pred==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FalsePositive\n",
    "TN = len(y[pred==y].index &y[pred==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([107, 119, 129, 134], dtype='int64')"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred!=y].index &y[pred==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FP = len(y[pred!=y].index &y[pred==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([51, 53, 54, 55, 56, 59, 61, 63, 64, 66, 70, 77, 78, 83, 84, 85, 86,\n",
       "            88, 89, 90, 91, 94, 95, 96, 98, 99],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred!=y].index &y[pred==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN = len(y[pred!=y].index &y[pred==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指標の抜け漏れがないか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y) == TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acuraccy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, pred):\n",
    "    #TruePositive\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    \n",
    "    accuracy = (TP + TN )/(TP+TN+FP+FN)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(y, pred):\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    \n",
    "    recall = TP /(TP+FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, pred):\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    precision = float(TP /(TP+FP ))\n",
    "#     precison = TP /(TP+FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_1(y,pred):\n",
    "    f_1 = (2*recall(y,pred)*precision(y,pred))/(recall(y,pred)*precision(y,pred))\n",
    "    return f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    def __init__(self, y,pred):\n",
    "        #教師データ\n",
    "        self.y = y\n",
    "        # 予測値\n",
    "        self.pred = pred\n",
    "        #TruePositive\n",
    "        self.TP = len(y[pred==y].index &y[pred==0].index)\n",
    "        #TrueNegative\n",
    "        self.TN = len(y[pred==y].index &y[pred==1].index)\n",
    "        #FalsePositive\n",
    "        self.FP  = len(y[pred!=y].index &y[pred==0].index)\n",
    "        #FalseNegative\n",
    "        self.FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        accuracy = (self.TP + self.TN )/(self.TP+self.TN+self.FP+self.FN)\n",
    "        return accuracy\n",
    "    \n",
    "    def recall(self):\n",
    "        recall = self.TP /(self.TP+self.FN)\n",
    "        return recall\n",
    "    \n",
    "    def precision(self):\n",
    "        precision = float(self.TP /(self.TP+self.FP ))\n",
    "        return precision\n",
    "    \n",
    "    def f_1(self):\n",
    "        f_1 = (2*self.recall()*self.precision())/(self.recall()*self.precision())\n",
    "        return f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通るか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = Metric(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.f_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    線形回帰\n",
    "    ＊コンストラクタ（__init__）のパラメータはここに書いておくと分かりやすい\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "#     num_iter = 3000\n",
    "#     lr= 0.01\n",
    "\n",
    "    def __init__(self, num_iter, lr):\n",
    "        #メソッド内で共有したい変数をおlく\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "#         self.bias = bias\n",
    "#         self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        #predictで使う重み\n",
    "        self.theta =  np.random.rand(4)\n",
    "\n",
    "    def sigmoid(self,X):\n",
    "        y = 1.0 / (1.0 + np.exp(-X))\n",
    "        return y      \n",
    "\n",
    "    def y_hat(self,X):\n",
    "        #y =  sigmoid(np.dot(theta,　X.T))\n",
    "        #仮定関数は、シグモイド関数の引数にθxを入れたもの\n",
    "        #theta= 4列\n",
    "        #X = 100*4\n",
    "        #(1*4 ) *(4*100)\n",
    "        # 100\n",
    "        y = self.sigmoid(np.dot(self.theta,X.T))\n",
    "        return y\n",
    "\n",
    "    def compute_cost(self,X,y,λ= 0.01):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # 第１項\n",
    "        one_one = np.dot((-1 * y), np.log(self.y_hat(X)))\n",
    "        one_two = np.dot((1-y),np.log(1-  self.y_hat(X)))\n",
    "        one = 1/len(X)*(one_one- one_two)\n",
    "        two = (λ/2*len(X))*sum(self.theta**2)\n",
    "        return one+two\n",
    "\n",
    "            \n",
    "    def  gradient_descent(self,X,y):\n",
    "\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            lr = 0.0001\n",
    "           #訓練データの誤差\n",
    "            past_costs = []\n",
    "            #重み\n",
    "            past_thetas = []\n",
    "            #平均二乗和誤差を計算する\n",
    "            loss = self.compute_cost(X,y)\n",
    "            #追加\n",
    "            past_costs.append(self.loss)\n",
    "            #追加\n",
    "            past_thetas.append(self.theta)\n",
    "\n",
    "            for i in range(5000):\n",
    "                #仮定関数\n",
    "                y_h=self.y_hat(X)\n",
    "\n",
    "                #パラメータの更新式\n",
    "                self.theta = self.theta - lr *(1/len(y))*(X.T.dot(y_h-y))\n",
    "                cost = (self.compute_cost(X,y))\n",
    "                # 訓練データの誤差を計算する\n",
    "                loss = self.compute_cost(X,y)\n",
    "                #訓練データの誤差を格納する\n",
    "                past_costs.append(loss)\n",
    "                past_thetas.append(self.theta)\n",
    "\n",
    "            self.loss = past_costs\n",
    "            self.theta = past_thetas\n",
    "\n",
    "            return  past_thetas,past_costs\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gradient_descent(X,y)\n",
    "#         if self.verbose:\n",
    "#             #verboseをTrueにした際は学習過程を出力\n",
    "#             print()\n",
    "        \n",
    "        \n",
    "    #返す値は他で使わないのでリターンで返す\n",
    "    def predict_probs(self,X):\n",
    "        return np.dot(self.theta[-1],X.T)\n",
    "\n",
    "\n",
    "    def predict(self,X,threshold=0.5):\n",
    "        pred = y_hat(X,theta)\n",
    "        for i in range(pred.size):\n",
    "            if pred[i]>= threshold:\n",
    "                pred[i] = 1\n",
    "            elif pred[i] < threshold:\n",
    "                pred[i] = 0\n",
    "        return pred.astype('int')\n",
    "    \n",
    "    def plot_loss_curve(self):\n",
    "        pd.DataFrame(self.theta).rename(columns={0:'Train'}).plot.line()\n",
    "        plt.title('LossCurve')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickleで重みを保存\n",
    "機械学習の学習済みモデルを回す方法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
