{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教師あり学習（分類問題、回帰問題）\n",
    "\n",
    "正解ラベル付きのトレーニングデータからモデルを学習し、未知のデータに対して予測を行う\n",
    "\n",
    "教師あり学習には分類問題と、回帰問題の２つがある。\n",
    "\n",
    "分類問題とは、カテゴリ別に分けてあるデータを学習し、未知のデータのカテゴリ(離散値)を予測する。このコンテンツではこの分類問題に対するアルゴリズムの理解や簡単な問題の実装を行う。\n",
    "\n",
    "回帰問題とは、分類問題と違って、こちらは連続値を予測をする。\n",
    "\n",
    "# 教師なし学習\n",
    "\n",
    "正解ラベルのついていないデータや構造が不明なデータに対し、データの構造や関係性を機械が見出すことを指す。\n",
    "\n",
    "\n",
    "\n",
    "# フィーチャースケーリング\n",
    "\n",
    "再急降下法では最小二乗法の最小値を求める必要がある。そのとき特徴量に対して、ある幅で最小値に近づいていく(収束する)必要があるが、その特徴量の範囲がそれぞれ異なると時間がかかってしまいまう。そのため、特徴量の範囲を調整することをfeature scaling(フィーチャースケーリング)と言い、それによって処理時間を短くすることができる。\n",
    "\n",
    "例えば、「特徴数１(x1)：大きさ」の場合、範囲が「1-3000」だったとして、「特徴数２(x2)：部屋数」の場合は、範囲が「1-3」だとしたら、同じ幅で最小値に収束していくには範囲が大きく異なるため、時間がかかってしまう。。正規化の種類として、**Min-Max normalization**と**z-score normalization(標準化)**がある。\n",
    "\n",
    "## Min-Max normalization\n",
    "\n",
    "Min-Max normalizationは最小値を０、最大値を１に変換する正規化の手法。\n",
    "\n",
    "この正規化条件は、データの分布が一様分布であるもの。\n",
    "\n",
    "$x_{min-max}^{i} = \\frac{ x^i - min }{ max - min }$\n",
    "\n",
    "\n",
    "\n",
    "これを以上の例にあてはめると\n",
    "\n",
    "$x_1 = \\frac{ 大きさ(1～3000) }{ 3000-1 },　  x_2 = \\frac{ 部屋数(1～3) }{ 3-1 }$\n",
    "\n",
    "\n",
    "\n",
    "以上の様な変換を行うことで、$0 \\leq {x_1} \\leq 1 ,　  0 \\leq x_2 \\leq 1$\n",
    "\n",
    "## z-score normalization(標準化)\n",
    "\n",
    "標準化は、元のデータの平均を０、標準偏差を１に変換する正規化の手法。\n",
    "\n",
    "$x_{z-score}^{i} = \\frac{x^i - \\mu}{\\sigma}$\n",
    "\n",
    "\n",
    "\n",
    "## 以上の使い分け\n",
    "\n",
    "データを得るための仮想的な分布の形状が異なる。\n",
    "\n",
    "Z-scoreはガウス分布を、Min-Maxは一様分布を仮定している。\n",
    "\n",
    "ガウス分布は、釣鐘型の形をしていて、平均と分散で分布全体を特徴付けることができる。\n",
    "\n",
    "また、分布の中心に近い値が観測されやすく、遠い値が観測されにくい。\n",
    "\n",
    "ガウス分布と異なり、一様分布では分布に含まれる各値が等しい確率で出現することを想定している。サンプルの中心からどんなに離れた値で合っても、それが観察される確率がある程度あるので、サンプルを得る過程でのエラーを検出できない。\n",
    "\n",
    "標準化は外れ値のあるデータの対して有効。\n",
    "\n",
    "# メリット\n",
    "\n",
    "データを一定の方法で変形し、例えば身長と体重みたいな次元が違うものに対しても、なんとかして同じような単位で取り扱えるようにして、計算や比較しやすくする。\n",
    "\n",
    "\n",
    "\n",
    "# $h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n   (x_0 = 1)\\\\ $\n",
    "\n",
    "# $θ_0$の役割\n",
    "\n",
    "\n",
    "誤差\n",
    "\n",
    "\n",
    "# 学習率はどの様な値から選択すべきか\n",
    "\n",
    "学習率を大きくすると、最適な値の周りで振動して、損失関数が増加する。\n",
    "![](https://camo.qiitausercontent.com/2a618584f669abf195e2bb2e775a2fa0808632b3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3137323235342f37666361646166622d386239342d663939622d326537612d6262613837656237336237382e706e67)  \n",
    "このような場合は、学習率を徐々に下げて調整していく。\n",
    "また学習率を小さくすると、計算時間がかかる。\n",
    "最小の計算時間を追求するのも重要\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 学習曲線\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 正規方程式 \n",
    " 勾配法では、反復を繰り返しながら、大域的最適を求めようとしていたが、正規方程式を使えば解析的に  $\\theta$を解くことが出来る。\n",
    "・何をしているのかというと、要は接線の傾きが0の$\\theta$を求めている\n",
    "→例1：1次元$(\\theta \\in \\mathbb{R})$の場合、\n",
    "$J(\\theta )=a \\theta ^2 + b \\theta +c$というコスト関数だった場合、\n",
    "$\\frac{d}{d \\theta}J(\\theta ) =0$\n",
    "となる$\\theta$を求めればいい\n",
    "\n",
    "→例2：n+1次元$(\\theta \\in \\mathbb{R}^{n+1})$の場合、(mはデータ数、nは変数の数)\n",
    "$J(\\theta )=\\frac{1}{2m} \\sum_{i=1}^{m} \\left(h_\\theta (x^{i})-y^{i} \\right)^2\n",
    "\\frac{\\partial }{\\partial \\theta _j} J(\\theta )=0 (for every j)$\n",
    "となる$\\theta _0 , \\theta _1, \\ldots , \\theta _n$を求めればいい\n",
    "\n",
    "・これを行列で表現してみる\n",
    "・m個の訓練データ$(x^{(1)},y^{(1)}), \\ldots ,(x^{(m)},y^{(m)})$、n個の変数（特性）が合った場合、\n",
    "$x^{(i)}=\\left[\\begin{array}{c} x_0^{(i)} \\\\ x_1^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\\\ \\end{array} \\right] \\in \\mathbb{R}^{n+1}\n",
    "※x_0^{(i)}=1$であることを忘れずに\n",
    "各訓練データは上記のように示せ、それを転置(Transpose)しながら配置すると、下記のようになる\n",
    "$X=\\left[\\begin{array}{ccc} - & (x^{(1)})^T & - \\\\ - & (x^{(2)})^T & - \\\\  & \\vdots &  \\\\ - & (x^{(m)})^T & - \\\\ \\end{array} \\right]$\n",
    "※この時、Xはdesign matrixとも呼ばれる\n",
    "$\\theta = (X^T X)^{-1} X^T y$\n",
    "で各$\\theta$の値が求められる\n",
    "・この正規方程式で求める場合、スケーリングは考えなくても大丈夫\n",
    "\n",
    "・これだけ聞くと正規方程式が最強に見えてくるが、ケースによっては勾配法の方が良い場合がある\n",
    "・m個の訓練データ、n個の変数があった場合、それぞれ以下の特徴がある\n",
    "\n",
    "勾配法の特徴\n",
    "・学習率を選択する必要がある\n",
    "・多くの反復が必要\n",
    "・nが大きくても上手く機能する\n",
    "\n",
    "正規方程式の特徴\n",
    "・学習率を選択する必要がない\n",
    "・反復は不要\n",
    "・$ (X^T X)^{-1} $を計算する必要がある\n",
    "→nxnの計算になり、大体$O(n^3)$の計算量になる\n",
    "・nが大きいと計算に時間がかかってしまう\n",
    "\n",
    "\n",
    "\n",
    "# 更新式の導出\n",
    "\n",
    "更新式$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "この式から導出$\\theta_j := \\theta_j - \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\\\$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 局所最適解の問題\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ロス関数が平均２乗和なので、下に凸のグラフになり大域的最適解を求めることができる。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰\n",
    "    ＊コンストラクタ（__init__）のパラメータはここに書いておくと分かりやすい\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "#     num_iter = 3000\n",
    "#     lr= 0.01\n",
    "\n",
    "    def __init__(self, num_iter, lr):\n",
    "        #メソッド内で共有したい変数をおlく\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "#         self.bias = bias\n",
    "#         self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        #predictで使う重み\n",
    "        self.theta =  np.random.rand(2)\n",
    "\n",
    "\n",
    "    def compute_cost(self,X,y):\n",
    "        y_hat= np.dot(self.theta,X.T)\n",
    "        loss = np.sum((y_hat - 2)**2)/(len(X)*2)\n",
    "        return loss\n",
    "    \n",
    "    def  _gradient_descent(self,X,y):\n",
    "            theta = np.random.rand(X.shape[1])\n",
    "           #訓練データの誤差\n",
    "            past_costs = []\n",
    "            #バリデーションデータの誤差\n",
    "#             val_past_costs =[]\n",
    "            #重み\n",
    "            past_thetas = []\n",
    "            #平均二乗和誤差を計算する\n",
    "            loss = self.compute_cost(X,y)\n",
    "            #バリデーションデータの平均２乗和誤差\n",
    "#             val_loss = compute_cost(x_val,y_val,theta)\n",
    "            #追加\n",
    "            past_costs.append(loss)\n",
    "#             val_past_costs.append(val_loss)\n",
    "            #追加\n",
    "            past_thetas.append(self.theta)\n",
    "\n",
    "            for i in range(self.iter):\n",
    "                #仮定関数\n",
    "                y_hat = np.dot(X, self.theta)\n",
    "#                 val_y_hat =  np.dot(x_val, theta)\n",
    "                #パラメータの更新式\n",
    "                self.theta =self. theta - (self.lr/len(y)) * np.dot(X.T, y_hat - y)\n",
    "                # 訓練データの誤差を計算する\n",
    "                self.loss = self.compute_cost(X,y)\n",
    "                #訓練データの誤差を格納する\n",
    "                past_costs.append(loss)\n",
    "                #バリデーションデータの誤差を計算する\n",
    "#                 val_loss = compute_cost(x_val,y_val,theta)\n",
    "                #バリデーションデータの誤差を格納する\n",
    "#                 val_past_costs.append(val_loss)\n",
    "                past_thetas.append(theta)\n",
    "\n",
    "            self.loss = past_costs\n",
    "            self.theta = past_thetas\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        self._gradient_descent(X,y)\n",
    "#         if self.verbose:\n",
    "#             #verboseをTrueにした際は学習過程を出力\n",
    "#             print()\n",
    "        \n",
    "        \n",
    "    #返す値は他で使わないのでリターンで返す\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        return np.dot(self.theta,np.array(X).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ScratchLinearRegression(num_iter=2000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.loc[:,['GrLivArea','YearBuilt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranmarusato/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/ranmarusato/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in square\n",
      "/Users/ranmarusato/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:72: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2198</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1362</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1694</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2090</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1774</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1077</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1040</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2324</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>912</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1494</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1253</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>854</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1004</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1296</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1114</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1339</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2376</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1108</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1795</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1060</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1060</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1600</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>900</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1704</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1600</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>520</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1838</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>958</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>968</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1792</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1126</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1537</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>864</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1932</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1236</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1725</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2555</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>848</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2007</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>952</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1422</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>913</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1188</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2090</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1346</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>630</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1792</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1578</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1072</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1140</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1221</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1647</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2073</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2340</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1078</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1256</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GrLivArea  YearBuilt\n",
       "0          1710       2003\n",
       "1          1262       1976\n",
       "2          1786       2001\n",
       "3          1717       1915\n",
       "4          2198       2000\n",
       "5          1362       1993\n",
       "6          1694       2004\n",
       "7          2090       1973\n",
       "8          1774       1931\n",
       "9          1077       1939\n",
       "10         1040       1965\n",
       "11         2324       2005\n",
       "12          912       1962\n",
       "13         1494       2006\n",
       "14         1253       1960\n",
       "15          854       1929\n",
       "16         1004       1970\n",
       "17         1296       1967\n",
       "18         1114       2004\n",
       "19         1339       1958\n",
       "20         2376       2005\n",
       "21         1108       1930\n",
       "22         1795       2002\n",
       "23         1060       1976\n",
       "24         1060       1968\n",
       "25         1600       2007\n",
       "26          900       1951\n",
       "27         1704       2007\n",
       "28         1600       1957\n",
       "29          520       1927\n",
       "...         ...        ...\n",
       "1430       1838       2005\n",
       "1431        958       1976\n",
       "1432        968       1927\n",
       "1433       1792       2000\n",
       "1434       1126       1977\n",
       "1435       1537       1962\n",
       "1436        864       1971\n",
       "1437       1932       2008\n",
       "1438       1236       1957\n",
       "1439       1725       1979\n",
       "1440       2555       1922\n",
       "1441        848       2004\n",
       "1442       2007       2008\n",
       "1443        952       1916\n",
       "1444       1422       2004\n",
       "1445        913       1966\n",
       "1446       1188       1962\n",
       "1447       2090       1995\n",
       "1448       1346       1910\n",
       "1449        630       1970\n",
       "1450       1792       1974\n",
       "1451       1578       2008\n",
       "1452       1072       2005\n",
       "1453       1140       2006\n",
       "1454       1221       2004\n",
       "1455       1647       1999\n",
       "1456       2073       1978\n",
       "1457       2340       1941\n",
       "1458       1078       1950\n",
       "1459       1256       1965\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
