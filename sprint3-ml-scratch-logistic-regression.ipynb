{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰とは \n",
    "ロジスティック回帰 は線形分離可能なデータの境界線を学習によって見つけてデータの分類を行なう手法。\n",
    "特徴としては境界線が直線になること。そのため、二項分類などクラスの少ないデータに用いられる。 また、データがクラスに分類される確率も計算することが可す。これらの特徴から主に「天気予報の降水確率」など、分類される確率を知りたい時に用いられる。\n",
    "\n",
    "\n",
    "# 交差エントロピー関数とは  \n",
    "情報の最小単位を1bitとして、情報量は確率の関数として,f(p)とあらわせるとする。\n",
    "独立な事象１と２が起きた時の確率Pは、それぞれ\n",
    "P = P1* P2であらわせる。\n",
    "この時の情報量は\n",
    "f(P)=f(P1)*f(P2)とすると、左辺の単位はビット、右辺の単位は（ビット）^2となって一致しなくなる。\n",
    "そこで\n",
    "f(P) = f(P1)+f(P2)となると単位が一致する。\n",
    "つまり、事象１と事象２が起きた時の情報量は事象１が起きたという情報と、事象２が起きたという情報の和であらわせるべきということ。\n",
    "つまり、f(P₁* P₂)＝f(P₁)＋f(P₂）という性質を持つことになる。\n",
    "これは、log(P₁×P₂)=log(P₁)＋log(P₂）という対数関数の性質である。\n",
    "f (p) = log(p)と表したいが、これだとpが小さくなるとf(p)が小さくねったしまう。\n",
    "確率が小さい事象が起きた時の情報量が大きいと考える方が自然なので\n",
    "f(p)＝-log(p)とマイナスをつける。情報エントロピーは何かデータを得た時の驚き具合と表現される。\n",
    "これが自己情報量とあらわせる。\n",
    "通常のエントロピーとはp×log(p)のようにlogの中身と外側に同じ変数が使われているのが普通のエントロピーである。\n",
    "それに対して、交差エントロピーとは、t×log(y)のようにlogの中身と外側に異なる変数が使われているものである。\n",
    "交差エントロピーとは、情報量の期待値であり、確率*実現値で定義できる。\n",
    "\n",
    "つまり、\n",
    "$ { \\displaystyle\n",
    "\\begin{eqnarray*}\n",
    "交差エントロピー&=&期待値 \\\\\n",
    " &=&\\sum_{x} 確率 * 実現値 \\\\\n",
    " &=&\\sum_{x} 確率 * 情報量 \\\\\n",
    " &=&\\sum_{x} p(x)*(-\\log q(x)) \\\\\n",
    " &=&-\\sum_{x} p(x)\\log q(x)\n",
    "\\end{eqnarray*}\n",
    "}\n",
    "$\n",
    "２値分類の誤差関数は、$ {\\log L = -\\sum_{i=1}^N \\left(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log(1-f_\\theta(x^{(i)}))\\right) $\n",
    "\n",
    "# シグモイド関数とは\n",
    "$ \\displaystyle f(x) = \\frac{1}{1+\\exp (-x)} $\n",
    "以上の関数は 0 < f(x) < 1の範囲となる。\n",
    "この関数に入力させることで、確率と同じような性質を与えることができる。\n",
    "# 正則化とは  \n",
    "学習をする際に、偏りすぎたデータにまで必要以上に対応してしまうと、過学習の状態に陥ることがある。\n",
    "過学習の状態は、与えた学習データに対して、小さな誤差となるモデルが構築できている。ただし、ごく一部の例外的なデータに過度に対応したモデルとなっているために、構築した学習モデルを未知データに適用すると必ずしも適切な予測値を返さない状態となる。\n",
    "学習データの中のごく一部の例外的なデータに過度に適用したモデルが構築されている状態ということで過学習という。\n",
    "そのため、極端な重みのデータに対してペナルティを与える正則化が用いられる。正則化とは、モデルを学習する際に、複雑さが増すことに対するペナルティを設け、このペナルティを訓練誤差に加えた量がもっとも小さくなる学習モデルを求めるように汎化性能を高めようとするもの。\n",
    "機械学習では、L1正則化とL2正則化が一般的に使用される。\n",
    "L1正則化は、ペナルティとして、学習モデルのパラメーターの絶対値の総和に用いるもので、L2正則化は、ペナルティとして学習モデルのパラメータの２乗の総和を用いるもので、以下のような特徴を持つ。\n",
    "- L1正則化　特定のデータの重みを0にする事で、不要なデータを削除する\n",
    "- L2正則化　データの大きさに応じて0に近づけて、滑らかなモデルとする\n",
    "\n",
    "# ロジスティック回帰では平均二乗誤差ではなく交差エントロピー誤差を使う理由  \n",
    "凸最適化をするため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://aidiary.hatenablog.com/entry/20140415/1397570262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranmarusato/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/Users/ranmarusato/.matplotlib/matplotlibrc\", line #629\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    y = 1.0 / (1.0 + np.exp(-X))\n",
    "    return y      \n",
    "\n",
    "def y_hat(X,theta):\n",
    "    #y =  sigmoid(np.dot(theta,　X.T))\n",
    "    #仮定関数は、シグモイド関数の引数にθxを入れたもの\n",
    "    #theta= 4列\n",
    "    #X = 100*4\n",
    "    #(1*4 ) *(4*100)\n",
    "    # 100\n",
    "    y = sigmoid(np.dot(theta,X.T))\n",
    "    return y\n",
    "\n",
    "def compute_cost(X,y,theta,λ= 0.01):\n",
    "    #numpy配列は計算が早いのでここで変換\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    #第１項\n",
    "    one_one = np.dot((-1 * y), np.log(y_hat(X,theta)))\n",
    "    one_two = np.dot((1-y),np.log(1-  y_hat(X,theta)))\n",
    "    one = 1/len(X)*(one_one- one_two)\n",
    "    #第２項\n",
    "    two = (λ/2*len(X))*sum(theta**2)\n",
    "    return one+two\n",
    "\n",
    "\n",
    "def  gradient_descent(X,y,theta):\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        lr = 0.0001\n",
    "       #訓練データの誤差\n",
    "        past_costs = []\n",
    "        #重み\n",
    "        past_thetas = []\n",
    "        #平均二乗和誤差を計算する\n",
    "        loss = compute_cost(X,y,theta)\n",
    "        #追加\n",
    "        past_costs.append(loss)\n",
    "        #追加\n",
    "        past_thetas.append(theta)\n",
    "\n",
    "        for i in range(5000):\n",
    "            #仮定関数\n",
    "            y_h=y_hat(X,theta)\n",
    "\n",
    "            #パラメータの更新式\n",
    "            theta = theta - lr *(1/len(y))*(X.T.dot(y_h-y))\n",
    "            # 訓練データの誤差を計算する\n",
    "            loss = compute_cost(X,y,theta)\n",
    "            #訓練データの誤差を格納する\n",
    "            past_costs.append(loss)\n",
    "            past_thetas.append(theta)\n",
    "\n",
    "        loss = past_costs\n",
    "        theta = past_thetas\n",
    "\n",
    "        return  past_thetas,past_costs\n",
    "    \n",
    "def predict_probs(X,theta):\n",
    "    return  y_hat(X,past_thetas[-1])\n",
    "\n",
    "\n",
    "def predict(X,theta,threshold=0.5):\n",
    "    pred = y_hat(X,theta)\n",
    "    for i in range(pred.size):\n",
    "        if pred[i]>= 0.5:\n",
    "            pred[i] = 1\n",
    "        elif pred[i] < 0.5:\n",
    "            pred[i] = 0\n",
    "    return pred.astype('int')\n",
    "\n",
    "def plot_loss_curve(past_costs):\n",
    "    pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "    plt.title('LossCurve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('../iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (iris['Species'][iris['Species']=='Iris-versicolor'].index)\n",
    "index = index.append( iris['Species'][iris['Species']=='Iris-virginica'].index)\n",
    "iris = iris.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'Iris-versicolor':0,'Iris-virginica':1}\n",
    "iris.Species = iris.Species.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.drop(['Id','Species'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = iris.Species\n",
    "theta = np.random.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " past_thetas,past_costs = gradient_descent(X,y,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(past_costs):\n",
    "    pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "    plt.title('LossCurve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranmarusato/anaconda/lib/python3.6/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(past_costs).rename(columns={0:'Train'}).plot.line()\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#正解率\n",
    "def accuracy_rate(y, t):\n",
    "    #データ型の確認の条件式\n",
    "    return np.sum(y == t)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4485927 , 0.51945162, 0.47234215, 0.47176259, 0.46416172,\n",
       "       0.54699409, 0.56794486, 0.48616884, 0.44867   , 0.56068541,\n",
       "       0.43897797, 0.53690572, 0.37358035, 0.5298848 , 0.5020443 ,\n",
       "       0.45212285, 0.59892705, 0.46764642, 0.42154828, 0.46083447,\n",
       "       0.62856753, 0.4550187 , 0.47666519, 0.50252977, 0.45148457,\n",
       "       0.45283871, 0.4331237 , 0.5090166 , 0.53479609, 0.4217622 ,\n",
       "       0.4537409 , 0.43860672, 0.46680283, 0.56414814, 0.62405297,\n",
       "       0.60329581, 0.48303156, 0.39883054, 0.55367926, 0.49673542,\n",
       "       0.53321613, 0.53449875, 0.46219303, 0.46052241, 0.52423188,\n",
       "       0.5409945 , 0.53596769, 0.4777937 , 0.45617179, 0.51565514,\n",
       "       0.72124102, 0.61138565, 0.55647294, 0.60258839, 0.63240235,\n",
       "       0.5455469 , 0.64124957, 0.52687902, 0.51689453, 0.65837444,\n",
       "       0.58968919, 0.54951586, 0.56455117, 0.59972751, 0.65746971,\n",
       "       0.6382686 , 0.58151703, 0.64361655, 0.52092572, 0.48668677,\n",
       "       0.60569999, 0.64009334, 0.50786981, 0.52396656, 0.62843575,\n",
       "       0.55381904, 0.54174455, 0.58704126, 0.5993353 , 0.49839978,\n",
       "       0.49288834, 0.58208284, 0.60646447, 0.52985343, 0.5628954 ,\n",
       "       0.50799707, 0.69950909, 0.60632848, 0.59222467, 0.55608578,\n",
       "       0.61854171, 0.54746399, 0.61138565, 0.63300124, 0.65579841,\n",
       "       0.5689791 , 0.51429105, 0.5730099 , 0.69107123, 0.6272609 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probs(X,past_thetas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X,past_thetas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各評価指標の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([50, 52, 53, 54, 57, 58, 60, 62, 65, 67, 68, 69, 71, 72, 74, 75, 76,\n",
       "            79, 80, 81, 82, 86, 87, 89, 92, 93, 97, 98],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred==y].index &y[pred==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TruePositive\n",
    "TP = len(y[pred==y].index &y[pred==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "            113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126,\n",
       "            127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "            142, 143, 144, 145, 146, 147, 148, 149],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred==y].index &y[pred==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FalsePositive\n",
    "TN = len(y[pred==y].index &y[pred==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([119, 129, 130], dtype='int64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred!=y].index &y[pred==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FP = len(y[pred!=y].index &y[pred==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([51, 55, 56, 59, 61, 63, 64, 66, 70, 73, 77, 78, 83, 84, 85, 88, 90,\n",
       "            91, 94, 95, 96, 99],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[pred!=y].index &y[pred==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN = len(y[pred!=y].index &y[pred==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 指標の抜け漏れがないか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y) == TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acuraccy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y, pred):\n",
    "    #TruePositive\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    \n",
    "    accuracy = (TP + TN )/(TP+TN+FP+FN)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(y, pred):\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    \n",
    "    recall = TP /(TP+FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y, pred):\n",
    "    TP = len(y[pred==y].index &y[pred==0].index)\n",
    "    #TrueNegative\n",
    "    TN = len(y[pred==y].index &y[pred==1].index)\n",
    "    #False Positive\n",
    "    FP = len(y[pred!=y].index &y[pred==0].index)\n",
    "    #Flase Negative\n",
    "    FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "    precision = float(TP /(TP+FP ))\n",
    "#     precison = TP /(TP+FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032258064516129"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_1(y,pred):\n",
    "    f_1 = (2*recall(y,pred)*precision(y,pred))/(recall(y,pred)*precision(y,pred))\n",
    "    return f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    def __init__(self, y,pred):\n",
    "        #教師データ\n",
    "        self.y = y\n",
    "        # 予測値\n",
    "        self.pred = pred\n",
    "        #TruePositive\n",
    "        self.TP = len(y[pred==y].index &y[pred==0].index)\n",
    "        #TrueNegative\n",
    "        self.TN = len(y[pred==y].index &y[pred==1].index)\n",
    "        #FalsePositive\n",
    "        self.FP  = len(y[pred!=y].index &y[pred==0].index)\n",
    "        #FalseNegative\n",
    "        self.FN = len(y[pred!=y].index &y[pred==1].index)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        accuracy = (self.TP + self.TN )/(self.TP+self.TN+self.FP+self.FN)\n",
    "        return accuracy\n",
    "    \n",
    "    def recall(self):\n",
    "        recall = self.TP /(self.TP+self.FN)\n",
    "        return recall\n",
    "    \n",
    "    def precision(self):\n",
    "        precision = float(self.TP /(self.TP+self.FP ))\n",
    "        return precision\n",
    "    \n",
    "    def f_1(self):\n",
    "        f_1 = (2*self.recall()*self.precision())/(self.recall()*self.precision())\n",
    "        return f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通るか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = Metric(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032258064516129"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.f_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    線形回帰\n",
    "    ＊コンストラクタ（__init__）のパラメータはここに書いておくと分かりやすい\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "#     num_iter = 3000\n",
    "#     lr= 0.01\n",
    "\n",
    "    def __init__(self, num_iter, lr):\n",
    "        #メソッド内で共有したい変数をおlく\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.bias = bias\n",
    "#         self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        #predictで使う重み\n",
    "        self.theta =  None\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        y = 1.0 / (1.0 + np.exp(-X))\n",
    "        return y      \n",
    "\n",
    "    def y_hat(self, X):\n",
    "        #y =  sigmoid(np.dot(theta,　X.T))\n",
    "        #仮定関数は、シグモイド関数の引数にθxを入れたもの\n",
    "        #theta= 4列\n",
    "        #X = 100*4\n",
    "        #(1*4 ) *(4*100)\n",
    "        # 100\n",
    "        y = self.sigmoid(np.dot(self.theta, X.T))\n",
    "        return y\n",
    "\n",
    "    def compute_cost(self, X, y, λ= 0.01):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # 第１項\n",
    "        one_one = np.dot((-1 * y), np.log(self.y_hat(X)))\n",
    "        one_two = np.dot((1-y),np.log(1-  self.y_hat(X)))\n",
    "        one = 1/len(X)*(one_one- one_two)\n",
    "        two = (λ/2*len(X))*sum(self.theta**2)\n",
    "        return one+two\n",
    "\n",
    "            \n",
    "    def  gradient_descent(self, X, y):\n",
    "\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            lr = 0.0001\n",
    "           #訓練データの誤差\n",
    "            #平均二乗和誤差を計算して追加\n",
    "            self.loss.append(self.compute_cost(X,y))\n",
    "            #追加\n",
    "            \n",
    "            past_thetas.append(self.theta)\n",
    "            self.theta = np.random.rand(X.shape[0])\n",
    "            for i in range(5000):\n",
    "                #仮定関数\n",
    "                y_h=self.y_hat(X)\n",
    "\n",
    "                #パラメータの更新式\n",
    "                self.theta = self.theta - lr *(1/len(y))*(X.T.dot(y_h-y))\n",
    "                #訓練データの誤差を格納する\n",
    "                self.loss.append(self.compute_cost(X,y))\n",
    "                self.theta.append(self.theta)\n",
    "\n",
    "  \n",
    "\n",
    "            return  past_thetas,self.loss\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gradient_descent(X, y)\n",
    "\n",
    "        \n",
    "        \n",
    "    #返す値は他で使わないのでリターンで返す\n",
    "    def predict_probs(self,X):\n",
    "             # バイアス項に合わせて整形\n",
    "        if(not(self.bias)):\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "        return np.dot(self.theta[-1], X.T)\n",
    "\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "             # バイアス項に合わせて整形\n",
    "        if(not(self.bias)):\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "        pred = y_hat(X,theta)\n",
    "        pred[pred>=threshold] = 1\n",
    "        pred[pred<threshold] = 0\n",
    "        return pred.astype('int')\n",
    "    \n",
    "    \n",
    "    def plot_loss_curve(self):\n",
    "        pd.DataFrame(self.theta).rename(columns={0:'Train'}).plot.line()\n",
    "        plt.title('LossCurve')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
